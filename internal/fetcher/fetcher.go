package fetcher

import (
	"context"
	"log"
	"sync"
	"time"
	"top-ai-news/internal/database"
	"top-ai-news/internal/model"
)

type Fetcher struct {
	db     *database.DB
	stopCh chan struct{}
	mu     sync.Mutex
}

func New(db *database.DB) *Fetcher {
	return &Fetcher{
		db:     db,
		stopCh: make(chan struct{}),
	}
}

// FetchAndStore fetches RSS feeds concurrently, ranks articles, and stores top 5 per category.
func (f *Fetcher) FetchAndStore(date string) error {
	f.mu.Lock()
	defer f.mu.Unlock()

	ctx, cancel := context.WithTimeout(context.Background(), 60*time.Second)
	defer cancel()

	allFeeds := append(DomesticFeeds(), GlobalFeeds()...)
	sourceWeights := BuildSourceWeights(allFeeds)

	// Fetch all feeds concurrently
	type fetchResult struct {
		source   FeedSource
		articles []RawArticle
		err      error
	}

	results := make(chan fetchResult, len(allFeeds))
	var wg sync.WaitGroup

	for _, feed := range allFeeds {
		wg.Add(1)
		go func(src FeedSource) {
			defer wg.Done()
			articles, err := FetchFeed(ctx, src)
			results <- fetchResult{source: src, articles: articles, err: err}
		}(feed)
	}

	// Close results channel when all goroutines finish
	go func() {
		wg.Wait()
		close(results)
	}()

	// Collect results, split by category
	var domestic, global []RawArticle
	for r := range results {
		if r.err != nil {
			log.Printf("âš  æŠ“å– %s å¤±è´¥: %v", r.source.Name, r.err)
			continue
		}
		log.Printf("âœ“ ä»Ž %s èŽ·å– %d ç¯‡æ–‡ç« ", r.source.Name, len(r.articles))
		for _, a := range r.articles {
			if a.Category == "domestic" {
				domestic = append(domestic, a)
			} else {
				global = append(global, a)
			}
		}
	}

	// Rank and select top 5 per category
	topDomestic := RankAndSelect(domestic, 5, sourceWeights)
	topGlobal := RankAndSelect(global, 5, sourceWeights)

	// Delete existing news for the date to avoid duplicates
	if err := f.db.DeleteNewsByDate(date); err != nil {
		return err
	}

	// Store results
	stored := 0
	for i, a := range topDomestic {
		n := rawToNews(a, "domestic", date, i+1)
		if _, err := f.db.InsertNews(n); err != nil {
			log.Printf("ä¿å­˜å›½å†…æ–°é—»å¤±è´¥: %v", err)
		} else {
			stored++
		}
	}
	for i, a := range topGlobal {
		n := rawToNews(a, "global", date, i+1)
		if _, err := f.db.InsertNews(n); err != nil {
			log.Printf("ä¿å­˜å…¨çƒæ–°é—»å¤±è´¥: %v", err)
		} else {
			stored++
		}
	}

	log.Printf("âœ“ å…±ä¿å­˜ %d æ¡æ–°é—» (å›½å†…: %d, å…¨çƒ: %d)", stored, len(topDomestic), len(topGlobal))
	return nil
}

// StartScheduler starts a background goroutine that fetches news on startup (if needed)
// and refreshes periodically at the given interval.
func (f *Fetcher) StartScheduler(interval time.Duration) {
	go func() {
		// Initial fetch: check if today has data
		today := time.Now().Format("2006-01-02")
		has, _ := f.db.HasNewsForDate(today)
		if !has {
			log.Println("ðŸ“¡ é¦–æ¬¡å¯åŠ¨ï¼Œæ­£åœ¨æ‹‰å–ä»Šæ—¥æ–°é—»...")
			if err := f.FetchAndStore(today); err != nil {
				log.Printf("é¦–æ¬¡æ‹‰å–æ–°é—»å¤±è´¥: %v", err)
			}
		} else {
			log.Println("âœ“ ä»Šæ—¥æ–°é—»å·²å­˜åœ¨ï¼Œè·³è¿‡é¦–æ¬¡æ‹‰å–")
		}

		ticker := time.NewTicker(interval)
		defer ticker.Stop()

		for {
			select {
			case <-ticker.C:
				date := time.Now().Format("2006-01-02")
				log.Printf("ðŸ“¡ å®šæ—¶åˆ·æ–°æ–°é—» (%s)...", date)
				if err := f.FetchAndStore(date); err != nil {
					log.Printf("å®šæ—¶æ‹‰å–æ–°é—»å¤±è´¥: %v", err)
				}
			case <-f.stopCh:
				log.Println("æ–°é—»è°ƒåº¦å™¨å·²åœæ­¢")
				return
			}
		}
	}()
}

// Stop gracefully shuts down the scheduler.
func (f *Fetcher) Stop() {
	close(f.stopCh)
}

// rawToNews converts a RawArticle to a model.News for database storage.
func rawToNews(a RawArticle, category, date string, rank int) model.News {
	return model.News{
		Title:       a.Title,
		Summary:     a.Summary,
		SourceURL:   a.SourceURL,
		SourceName:  a.SourceName,
		Category:    category,
		PublishDate: date,
		Rank:        rank,
	}
}
